{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72fb9e5a",
   "metadata": {},
   "source": [
    "## Dataset preprocessing\n",
    "This section outlines the steps taken to preprocess the dataset before using any model. Proper preprocessing is crucial for ensuring that the data is clean, consistent, and suitable for analysis.\n",
    "The dataset seems pretty clean, but we performed the following preprocessing steps to ensure data quality:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7f3b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596dfe41",
   "metadata": {},
   "source": [
    "Replace \"-\" with 0 in the 'Gen' column to handle missing generation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8cfd3a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_gen_column(csv_path):\n",
    "    df = pd.read_csv(csv_path, dtype=str)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    if \"Gen\" not in df.columns:\n",
    "        print(f\"'Gen' column not found in {csv_path}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Replace cells that are exactly \"-\" (allow surrounding spaces) with \"0\"\n",
    "    df[\"Gen\"] = df[\"Gen\"].replace(r\"^\\s*-\\s*$\", \"0\", regex=True)\n",
    "\n",
    "    # Trim whitespace and convert to numeric, non-numeric -> NaN -> fill with 0\n",
    "    df[\"Gen\"] = pd.to_numeric(df[\"Gen\"].str.strip(), errors=\"coerce\").fillna(0)\n",
    "\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(\"Fixed Gen column and saved:\", csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6431d2",
   "metadata": {},
   "source": [
    "**Data Type Conversion**: Convert the relevant columns to numeric types to facilitate mathematical operations and analysis. Remove $ signs and commas before conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "463d480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dollar(csv_path):\n",
    "    data_hist = pd.read_csv(csv_path)\n",
    "\n",
    "    if 'RT Busbar' not in data_hist.columns or 'RT Hub' not in data_hist.columns or \\\n",
    "       'DA Busbar' not in data_hist.columns or 'DA Hub' not in data_hist.columns or \\\n",
    "       'P/OP' not in data_hist.columns:\n",
    "        print(f\"One or more required columns not found in {csv_path}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    data_hist['RT Busbar'] = data_hist['RT Busbar'].astype(str).str.replace(r'[\\(,]', '-', regex=True)\n",
    "    data_hist['RT Busbar'] = data_hist['RT Busbar'].astype(str).str.replace(r'[\\),]', '', regex=True)\n",
    "    \n",
    "    data_hist['RT Busbar'] = data_hist['RT Busbar'].astype(str).str.replace(r'[\\$,]', '', regex=True)\n",
    "    data_hist['RT Busbar'] = pd.to_numeric(data_hist['RT Busbar'], errors='coerce')\n",
    "    \n",
    "    data_hist['RT Hub'] = data_hist['RT Hub'].astype(str).str.replace(r'[\\$,]', '', regex=True)\n",
    "    data_hist['RT Hub'] = pd.to_numeric(data_hist['RT Hub'], errors='coerce')\n",
    "    \n",
    "    \n",
    "    data_hist['DA Busbar'] = data_hist['DA Busbar'].astype(str).str.replace(r'[\\(,]', '-', regex=True)\n",
    "    data_hist['DA Busbar'] = data_hist['DA Busbar'].astype(str).str.replace(r'[\\),]', '', regex=True)\n",
    "    \n",
    "    data_hist['DA Busbar'] = data_hist['DA Busbar'].astype(str).str.replace(r'[\\$,]', '', regex=True)\n",
    "    data_hist['DA Busbar'] = pd.to_numeric(data_hist['DA Busbar'], errors='coerce')\n",
    "    \n",
    "    data_hist['DA Hub'] = data_hist['DA Hub'].astype(str).str.replace(r'[\\$,]', '', regex=True)\n",
    "    data_hist['DA Hub'] = pd.to_numeric(data_hist['DA Hub'], errors='coerce')\n",
    "    \n",
    "    data_hist['P/OP'] = data_hist['P/OP'].astype(str).str.replace('OP' , '0', regex=True)\n",
    "    data_hist['P/OP'] = data_hist['P/OP'].astype(str).str.replace('P', '1', regex=True)\n",
    "    data_hist['P/OP'] = pd.to_numeric(data_hist['P/OP'], errors='coerce')\n",
    "\n",
    "    data_hist.to_csv(csv_path, index=False)\n",
    "    print(\"Converted dollar columns and saved:\", csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60853635",
   "metadata": {},
   "source": [
    "**Handling Missing Values**: Check for any missing values in the dataset. If any found, just drop those rows to maintain data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5538f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing_values(csv_path):  \n",
    "    df = pd.read_csv(csv_path)  \n",
    "    df.dropna(inplace=True)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(\"Dropped rows with missing values and saved:\", csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa11ffa",
   "metadata": {},
   "source": [
    "Add necessary columns as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b93120e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if 'Date' not in df.columns or 'HE' not in df.columns or 'Gen' not in df.columns or \\\n",
    "       'RT Busbar' not in df.columns or 'DA Busbar' not in df.columns or \\\n",
    "       'RT Hub' not in df.columns or 'DA Hub' not in df.columns:\n",
    "        print(f\"One or more required columns not found in {csv_path}. Skipping.\")\n",
    "        return -1\n",
    "\n",
    "    # Create new features\n",
    "    date = pd.to_datetime(df['Date'], format='%d-%b-%y')\n",
    "    df['Month'] = date.dt.month\n",
    "    df['Season'] = df['Month'].apply(lambda x: (x%12 + 3)//3)\n",
    "    season_dummies = pd.get_dummies(df['Season'], prefix='Season')\n",
    "    df = pd.concat([df, season_dummies], axis=1)\n",
    "    df.drop(columns=['Month', 'Season'], inplace=True)  # drop intermediate columns\n",
    "\n",
    "    df['day_of_week'] = date.dt.dayofweek\n",
    "    df['day_of_year'] = date.dt.dayofyear\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "    # add vacation indicator\n",
    "    # NERC holidays: New Year's Day, Memorial Day, Labor Day, Thanksgiving, and Christmas    \n",
    "    df['is_vacation'] = date.apply(lambda x: 1 if \n",
    "                                   (x.month == 1 and x.day == 1) or\n",
    "                                   (x.month == 5 and x.day >= 25 and x.day <= 31 and x.weekday() == 0) or\n",
    "                                   (x.month == 9 and x.day >= 1 and x.day <= 7 and x.weekday() == 0) or\n",
    "                                   (x.month == 11 and x.day >= 22 and x.day <= 28 and x.weekday() == 3) or\n",
    "                                   (x.month == 12 and x.day >= 24 and x.day <= 31) else 0)\n",
    "    \n",
    "    \n",
    "    # one-hot encode the HE, first split it to period of day\n",
    "    df['Period_of_Day'] = pd.cut(df['HE'], bins=[0, 6, 12, 18, 24], labels=['Night', 'Morning', 'Afternoon', 'Evening'], right=False)\n",
    "    pod_dummies = pd.get_dummies(df['Period_of_Day'], prefix='POD')\n",
    "    df = pd.concat([df, pod_dummies], axis=1)\n",
    "    df.drop(columns=['Period_of_Day'], inplace=True)\n",
    "    df.drop(columns=['HE'], inplace=True)  # drop HE as it's now encoded\n",
    "\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(\"Added features and saved:\", csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77555d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns(csv_path, columns_to_remove):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.drop(columns=columns_to_remove, inplace=True, errors='ignore')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(\"Removed specified columns and saved:\", csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f205f50",
   "metadata": {},
   "source": [
    "Preprocess all the CSV files in the data directory to ensure consistency across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2b2ca19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Gen' column not found in ../data/CAISO-Forward-Prices.csv. Skipping.\n",
      "Dropped rows with missing values and saved: ../data/CAISO-Forward-Prices.csv\n",
      "One or more required columns not found in ../data/CAISO-Forward-Prices.csv. Skipping.\n",
      "One or more required columns not found in ../data/CAISO-Forward-Prices.csv. Skipping.\n",
      "Fixed Gen column and saved: ../data/CAISO-Historical-Data.csv\n",
      "Dropped rows with missing values and saved: ../data/CAISO-Historical-Data.csv\n",
      "Converted dollar columns and saved: ../data/CAISO-Historical-Data.csv\n",
      "Added features and saved: ../data/CAISO-Historical-Data.csv\n",
      "'Gen' column not found in ../data/MISO-Forward-Prices.csv. Skipping.\n",
      "Dropped rows with missing values and saved: ../data/MISO-Forward-Prices.csv\n",
      "One or more required columns not found in ../data/MISO-Forward-Prices.csv. Skipping.\n",
      "One or more required columns not found in ../data/MISO-Forward-Prices.csv. Skipping.\n",
      "Fixed Gen column and saved: ../data/MISO-Historical-Data.csv\n",
      "Dropped rows with missing values and saved: ../data/MISO-Historical-Data.csv\n",
      "Converted dollar columns and saved: ../data/MISO-Historical-Data.csv\n",
      "Added features and saved: ../data/MISO-Historical-Data.csv\n",
      "'Gen' column not found in ../data/ERCOT-Forward-Prices.csv. Skipping.\n",
      "Dropped rows with missing values and saved: ../data/ERCOT-Forward-Prices.csv\n",
      "One or more required columns not found in ../data/ERCOT-Forward-Prices.csv. Skipping.\n",
      "One or more required columns not found in ../data/ERCOT-Forward-Prices.csv. Skipping.\n",
      "Fixed Gen column and saved: ../data/ERCOT-Historical-Data.csv\n",
      "Dropped rows with missing values and saved: ../data/ERCOT-Historical-Data.csv\n",
      "Converted dollar columns and saved: ../data/ERCOT-Historical-Data.csv\n",
      "Added features and saved: ../data/ERCOT-Historical-Data.csv\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data\"\n",
    "# traverse all csv files in data directory and apply fixes\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        csv_path = os.path.join(data_dir, filename)\n",
    "        fix_gen_column(csv_path)\n",
    "        drop_missing_values(csv_path)\n",
    "        convert_dollar(csv_path)\n",
    "        add_features(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5738f6d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
