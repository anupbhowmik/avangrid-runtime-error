{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43180034",
   "metadata": {},
   "source": [
    "## Predict Expected generation \n",
    "Use moving average, seasonal features, lag features, and cyclic encoding for hour of day. use other statistical model as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "46558792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a188a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prophet(df):\n",
    "    '''\n",
    "    Prophet automatically handles seasonality (daily, yearly, etc.) and trends.\n",
    "    We do not need to include engineered features like lags, rolling averages, or cyclic encodings in the input DataFrame for Prophet.\n",
    "    '''\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%d-%b-%y')\n",
    "    prophet_data = df[['Date','Gen']].rename(columns={'Date':'ds','Gen':'y'})\n",
    "    prophet = Prophet(daily_seasonality=True, yearly_seasonality=True)\n",
    "    prophet.fit(prophet_data)\n",
    "    forecast = prophet.predict(prophet_data)\n",
    "\n",
    "    df['prophet_pred'] = forecast['yhat']  # seasonality + trend baseline\n",
    "    df['residual'] = df['Gen'] - df['prophet_pred']\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17da695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgboost(df):\n",
    "    from xgboost import XGBRegressor\n",
    "    \n",
    "    res_features = ['HE','is_weekend','P/OP', 'is_vacation']\n",
    "    X = df[res_features]\n",
    "    y = df['residual']\n",
    "    # y = df['Gen']\n",
    "\n",
    "    # test-train split\n",
    "    split_index = int(len(df) * 0.8)\n",
    "    X_train, X_test = X.iloc[:split_index], X.iloc[split_index:] \n",
    "    y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "    # y_train_adj = np.where(y_train == 0, 1e-6, y_train)\n",
    "\n",
    "    reg = XGBRegressor(\n",
    "        objective='binary:logistic',\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=500\n",
    "    )\n",
    "\n",
    "    reg.fit(X_train, y_train)\n",
    "    df.loc[X_test.index, 'xgb_pred'] = reg.predict(X_test)\n",
    "    df['final_pred'] = df['prophet_pred'] + df['xgb_pred']\n",
    "\n",
    "    # find accuracy for XGBoost residual model\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "    mae = mean_absolute_error(df.loc[X_test.index, 'Gen'], df.loc[X_test.index, 'final_pred'])\n",
    "    print(f\"MAE: {mae}\")\n",
    "    \n",
    "    return df, reg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce8d2e7",
   "metadata": {},
   "source": [
    "## Predict for next 5 years\n",
    "Expected generation (by month, peak and off-peak periods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "798bdbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction_pipeline(df):\n",
    "    df_prophet = run_prophet(df)\n",
    "    df_xgb, reg_xgb = run_xgboost(df_prophet)\n",
    "\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plot_importance(reg_xgb, ax=plt.gca())\n",
    "    plt.title(\"XGBoost Feature Importance\")\n",
    "    plt.show()\n",
    "\n",
    "    # Create future dates (hourly for 5 years)\n",
    "    # start from 1 january 2026\n",
    "    future_dates = pd.date_range(\n",
    "        start=pd.Timestamp('2026-01-01 00:00:00'),\n",
    "        end=pd.Timestamp('2026-01-01 00:00:00') + pd.DateOffset(years=5),\n",
    "        freq='h'\n",
    "    )\n",
    "    \n",
    "    future_df = pd.DataFrame({'Date': future_dates})\n",
    "\n",
    "    # Add required features (example, adjust as needed)\n",
    "    future_df['HE'] = future_df['Date'].dt.hour\n",
    "\n",
    "    # NERC holidays: New Year's Day, Memorial Day, Labor Day, Thanksgiving, and Christmas\n",
    "    # add vacation indicator\n",
    "    future_df['NERC_holiday'] = future_df['Date'].apply(\n",
    "        lambda x: 1 if (x.month == 1 and x.day == 1) or\n",
    "                         (x.month == 5 and x.day >= 25 and x.day <= 31 and x.weekday() == 0) or\n",
    "                         (x.month == 9 and x.day >= 1 and x.day <= 7 and x.weekday() == 0) or\n",
    "                         (x.month == 11 and x.month == 11 and x.day >= 22 and x.day <= 28 and x.weekday() == 3) or\n",
    "                         (x.month == 12 and x.day == 25) else 0\n",
    "    )\n",
    "    \n",
    "    # Peak hours 0 = off-peak, 1 = peak\n",
    "    # Peak (P) hours = Mon-Fri, HE 7-22 excl NERC holidays\n",
    "    future_df['P/OP'] = future_df.apply(\n",
    "        lambda row: 1 if (row['Date'].weekday() < 5 and 7 <= row['Date'].hour <= 22 and row['NERC_holiday'] == 0) else 0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    future_df['day_of_week'] = future_df['Date'].dt.dayofweek\n",
    "    future_df['is_weekend'] = future_df['day_of_week'].isin([5,6]).astype(int)\n",
    "\n",
    "    # use Prophet to predict baseline generation\n",
    "    prophet_data = future_df[['Date']].rename(columns={'Date':'ds'})\n",
    "    forecast_future = Prophet(daily_seasonality=True, yearly_seasonality=True).fit(\n",
    "        df[['Date','Gen']].rename(columns={'Date':'ds','Gen':'y'})\n",
    "    ).predict(prophet_data)\n",
    "    future_df['prophet_pred'] = forecast_future['yhat']\n",
    "\n",
    "    # use XGBoost to predict residuals\n",
    "    res_features = ['HE','day_of_week','is_weekend','P/OP']\n",
    "    X_future = future_df[res_features]\n",
    "    future_df['xgb_pred'] = reg_xgb.predict(X_future)\n",
    "    future_df['final_pred'] = future_df['prophet_pred'] + future_df['xgb_pred']\n",
    "    return future_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "45bdbb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:54:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "03:54:14 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[03:54:15] /Users/runner/work/xgboost/xgboost/src/objective/regression_obj.cu:119: Check failed: is_valid: base_score must be in (0,1) for the logistic loss.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000012df752dc dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x000000012e272588 xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::ProbToMargin(xgboost::linalg::Tensor<float, 1>*) const + 956\n  [bt] (2) 3   libxgboost.dylib                    0x000000012e1d4124 xgboost::(anonymous namespace)::Intercept::InitModelParam(xgboost::LearnerTrainParam const&, bool) + 220\n  [bt] (3) 4   libxgboost.dylib                    0x000000012e1c663c xgboost::LearnerImpl::UpdateOneIter(int, std::__1::shared_ptr<xgboost::DMatrix>) + 708\n  [bt] (4) 5   libxgboost.dylib                    0x000000012df9e314 XGBoosterUpdateOneIter + 144\n  [bt] (5) 6   libffi.8.dylib                      0x00000001051e804c ffi_call_SYSV + 76\n  [bt] (6) 7   libffi.8.dylib                      0x00000001051e5834 ffi_call_int + 1404\n  [bt] (7) 8   _ctypes.cpython-312-darwin.so       0x000000010620c284 _ctypes_callproc + 1044\n  [bt] (8) 9   _ctypes.cpython-312-darwin.so       0x0000000106206404 PyCFuncPtr_call + 212\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXGBoostError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m data_dir = \u001b[33m\"\u001b[39m\u001b[33m../data\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m caiso_historical_df = pd.read_csv(data_dir + \u001b[33m\"\u001b[39m\u001b[33m/CAISO-Historical-Data.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m prediction = \u001b[43mrun_prediction_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaiso_historical_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[123]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mrun_prediction_pipeline\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_prediction_pipeline\u001b[39m(df):\n\u001b[32m      2\u001b[39m     df_prophet = run_prophet(df)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     df_xgb, reg_xgb = \u001b[43mrun_xgboost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_prophet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Plot feature importance\u001b[39;00m\n\u001b[32m      6\u001b[39m     plt.figure(figsize=(\u001b[32m8\u001b[39m,\u001b[32m6\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[122]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mrun_xgboost\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# y_train_adj = np.where(y_train == 0, 1e-6, y_train)\u001b[39;00m\n\u001b[32m     15\u001b[39m reg = XGBRegressor(\n\u001b[32m     16\u001b[39m     objective=\u001b[33m'\u001b[39m\u001b[33mbinary:logistic\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     17\u001b[39m     learning_rate=\u001b[32m0.05\u001b[39m,\n\u001b[32m     18\u001b[39m     n_estimators=\u001b[32m500\u001b[39m\n\u001b[32m     19\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mreg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m df.loc[X_test.index, \u001b[33m'\u001b[39m\u001b[33mxgb_pred\u001b[39m\u001b[33m'\u001b[39m] = reg.predict(X_test)\n\u001b[32m     23\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mfinal_pred\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mprophet_pred\u001b[39m\u001b[33m'\u001b[39m] + df[\u001b[33m'\u001b[39m\u001b[33mxgb_pred\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hackathon/lib/python3.12/site-packages/xgboost/core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hackathon/lib/python3.12/site-packages/xgboost/sklearn.py:1365\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1363\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1380\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hackathon/lib/python3.12/site-packages/xgboost/core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hackathon/lib/python3.12/site-packages/xgboost/training.py:199\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hackathon/lib/python3.12/site-packages/xgboost/core.py:2433\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2430\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2433\u001b[39m     \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2434\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2435\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2436\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2439\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/hackathon/lib/python3.12/site-packages/xgboost/core.py:323\u001b[39m, in \u001b[36m_check_call\u001b[39m\u001b[34m(ret)\u001b[39m\n\u001b[32m    312\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[32m    313\u001b[39m \n\u001b[32m    314\u001b[39m \u001b[33;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    return value from API calls\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret != \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "\u001b[31mXGBoostError\u001b[39m: [03:54:15] /Users/runner/work/xgboost/xgboost/src/objective/regression_obj.cu:119: Check failed: is_valid: base_score must be in (0,1) for the logistic loss.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000012df752dc dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x000000012e272588 xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::ProbToMargin(xgboost::linalg::Tensor<float, 1>*) const + 956\n  [bt] (2) 3   libxgboost.dylib                    0x000000012e1d4124 xgboost::(anonymous namespace)::Intercept::InitModelParam(xgboost::LearnerTrainParam const&, bool) + 220\n  [bt] (3) 4   libxgboost.dylib                    0x000000012e1c663c xgboost::LearnerImpl::UpdateOneIter(int, std::__1::shared_ptr<xgboost::DMatrix>) + 708\n  [bt] (4) 5   libxgboost.dylib                    0x000000012df9e314 XGBoosterUpdateOneIter + 144\n  [bt] (5) 6   libffi.8.dylib                      0x00000001051e804c ffi_call_SYSV + 76\n  [bt] (6) 7   libffi.8.dylib                      0x00000001051e5834 ffi_call_int + 1404\n  [bt] (7) 8   _ctypes.cpython-312-darwin.so       0x000000010620c284 _ctypes_callproc + 1044\n  [bt] (8) 9   _ctypes.cpython-312-darwin.so       0x0000000106206404 PyCFuncPtr_call + 212\n\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data\"\n",
    "caiso_historical_df = pd.read_csv(data_dir + \"/CAISO-Historical-Data.csv\")\n",
    "prediction = run_prediction_pipeline(caiso_historical_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb7df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.to_csv(data_dir + \"/CAISO-Future-Predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca35344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.99600684540787"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how the percentage have negative values\n",
    "negative_percentage = (prediction[prediction['final_pred'] < 0].shape[0] / prediction.shape[0]) * 100\n",
    "negative_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1064eb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>HE</th>\n",
       "      <th>NERC_holiday</th>\n",
       "      <th>P/OP</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>prophet_pred</th>\n",
       "      <th>xgb_pred</th>\n",
       "      <th>final_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>31.372285</td>\n",
       "      <td>-33.208401</td>\n",
       "      <td>-1.836116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-01-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>24.600462</td>\n",
       "      <td>-33.208401</td>\n",
       "      <td>-8.607939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-01-01 02:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9.276389</td>\n",
       "      <td>-33.208401</td>\n",
       "      <td>-23.932012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-01-01 03:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.884887</td>\n",
       "      <td>-33.208401</td>\n",
       "      <td>-37.093288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-01-01 04:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.413475</td>\n",
       "      <td>-33.208401</td>\n",
       "      <td>-40.621876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43820</th>\n",
       "      <td>2030-12-31 20:00:00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31.822641</td>\n",
       "      <td>-29.683079</td>\n",
       "      <td>2.139562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43821</th>\n",
       "      <td>2030-12-31 21:00:00</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.366422</td>\n",
       "      <td>-33.026001</td>\n",
       "      <td>2.340421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43822</th>\n",
       "      <td>2030-12-31 22:00:00</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>48.550344</td>\n",
       "      <td>-33.077694</td>\n",
       "      <td>15.472650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43823</th>\n",
       "      <td>2030-12-31 23:00:00</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>63.904278</td>\n",
       "      <td>-33.263466</td>\n",
       "      <td>30.640812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43824</th>\n",
       "      <td>2031-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>70.712893</td>\n",
       "      <td>-32.602394</td>\n",
       "      <td>38.110499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43825 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Date  HE  NERC_holiday  P/OP  day_of_week  is_weekend  \\\n",
       "0     2026-01-01 00:00:00   0             1     0            3           0   \n",
       "1     2026-01-01 01:00:00   1             1     0            3           0   \n",
       "2     2026-01-01 02:00:00   2             1     0            3           0   \n",
       "3     2026-01-01 03:00:00   3             1     0            3           0   \n",
       "4     2026-01-01 04:00:00   4             1     0            3           0   \n",
       "...                   ...  ..           ...   ...          ...         ...   \n",
       "43820 2030-12-31 20:00:00  20             0     1            1           0   \n",
       "43821 2030-12-31 21:00:00  21             0     1            1           0   \n",
       "43822 2030-12-31 22:00:00  22             0     1            1           0   \n",
       "43823 2030-12-31 23:00:00  23             0     0            1           0   \n",
       "43824 2031-01-01 00:00:00   0             1     0            2           0   \n",
       "\n",
       "       prophet_pred   xgb_pred  final_pred  \n",
       "0         31.372285 -33.208401   -1.836116  \n",
       "1         24.600462 -33.208401   -8.607939  \n",
       "2          9.276389 -33.208401  -23.932012  \n",
       "3         -3.884887 -33.208401  -37.093288  \n",
       "4         -7.413475 -33.208401  -40.621876  \n",
       "...             ...        ...         ...  \n",
       "43820     31.822641 -29.683079    2.139562  \n",
       "43821     35.366422 -33.026001    2.340421  \n",
       "43822     48.550344 -33.077694   15.472650  \n",
       "43823     63.904278 -33.263466   30.640812  \n",
       "43824     70.712893 -32.602394   38.110499  \n",
       "\n",
       "[43825 rows x 9 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da139a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
